"_id","slug","title","summary","pageUrl","author","authorSlug","karma","voteCount","commentsCount","postedAt","wordCount"
"NBgpPaz5vYhkr6GEc","debate-ai-safety-or-ai-control","Debate: AI safety or AI control?","This post presents a debate between focusing on AI safety (ensuring AI systems are fundamentally safe and aligned) versus AI control (developing methods to monitor and constrain AI systems). The debate explores different threat models, timelines, and strategic considerations for reducing AI existential risk.","https://www.alignmentforum.org/posts/NBgpPaz5vYhkr6GEc/debate-ai-safety-or-ai-control","Paul Christiano","paul-christiano","127","48","23","2024-11-20T10:30:00.000Z","4200"
"h9KPXqwKnJ7dMxZ3r","risks-from-learned-optimization","Risks from Learned Optimization in Advanced Machine Learning Systems","This post analyzes the risks that emerge when machine learning systems develop mesa-optimizersâ€”learned algorithms that perform optimization internally. We examine how base optimizers training learned optimizers can lead to misalignment between the mesa-objective and the base objective, creating potential safety hazards.","https://www.alignmentforum.org/posts/h9KPXqwKnJ7dMxZ3r/risks-from-learned-optimization","Evan Hubinger","evan-hubinger","203","87","45","2024-10-15T14:22:00.000Z","8900"
"FkgsxrGf3QxhfLWHG","weak-to-strong-generalization","Weak-to-Strong Generalization","Can weak models supervise strong models? This post explores the surprising finding that weak supervisors can elicit strong capabilities from more powerful models, with implications for scalable oversight and alignment of superhuman AI systems.","https://www.alignmentforum.org/posts/FkgsxrGf3QxhfLWHG/weak-to-strong-generalization","Collin Burns","collin-burns","156","63","28","2024-09-08T09:15:00.000Z","3600"
"5bd75cc58225bf0670370736","understanding-ai-alignment","Understanding AI Alignment","An introduction to the AI alignment problem: ensuring that increasingly capable AI systems reliably do what we want them to do. This post covers key concepts including value learning, corrigibility, scalable oversight, and the challenges of specifying human values.","https://www.alignmentforum.org/posts/5bd75cc58225bf0670370736/understanding-ai-alignment","Rob Bensinger","rob-bensinger","94","38","15","2024-08-12T11:45:00.000Z","2800"
"xqkGmBkrvH8TvJJuC","mechanistic-interpretability-challenges","Challenges in Mechanistic Interpretability","Mechanistic interpretability aims to reverse engineer neural networks to understand their internal algorithms. This post discusses current challenges including polysemanticity, distributed representations, and the difficulty of scaling interpretability methods to frontier models.","https://www.alignmentforum.org/posts/xqkGmBkrvH8TvJJuC/mechanistic-interpretability-challenges","Chris Olah","chris-olah","178","72","34","2024-07-25T16:30:00.000Z","5100"
"6Fpvch8RR29qLEWNH","deceptive-alignment","Deceptive Alignment in Future AI Systems","This post examines the possibility of deceptive alignment, where an AI system appears aligned during training but pursues a different objective when deployed. We analyze the conditions under which deception might emerge and potential countermeasures.","https://www.alignmentforum.org/posts/6Fpvch8RR29qLEWNH/deceptive-alignment","Evan Hubinger","evan-hubinger","215","91","52","2024-06-18T13:20:00.000Z","6700"
"vJFdjigzmcXMhNTsx","cooperative-ai-research","Cooperative AI: Foundations and Applications","Research on enabling AI systems to cooperate effectively with humans and other AI systems. This includes work on bargaining, coordination, and preventing conflicts in multi-agent systems.","https://www.alignmentforum.org/posts/vJFdjigzmcXMhNTsx/cooperative-ai-research","Jesse Clifton","jesse-clifton","82","31","12","2024-05-30T10:00:00.000Z","3200"
"2mhFMgtAjFJesaSYR","eliciting-latent-knowledge","Eliciting Latent Knowledge (ELK)","The ELK problem: how can we train a model to report what it actually believes rather than what will most impress human evaluators? This post introduces the problem and explores potential approaches including unsupervised learning and consistency checks.","https://www.alignmentforum.org/posts/2mhFMgtAjFJesaSYR/eliciting-latent-knowledge","Paul Christiano","paul-christiano","189","78","41","2024-04-22T15:40:00.000Z","5800"
"qHCDysDnvhteW7kRd","ai-safety-via-debate","AI Safety via Debate","Training AI systems to be aligned by having them debate each other with a human judge. This approach could help with scalable oversight by breaking down complex questions into smaller debatable claims.","https://www.alignmentforum.org/posts/qHCDysDnvhteW7kRd/ai-safety-via-debate","Geoffrey Irving","geoffrey-irving","143","59","27","2024-03-15T12:10:00.000Z","4100"
"BbvsoHDoRXuRCs5KZ","recursive-reward-modeling","Recursive Reward Modeling","An approach to scalable oversight where we recursively decompose tasks and use models to help evaluate subtasks. This could allow humans to oversee AI systems solving problems beyond human ability to directly evaluate.","https://www.alignmentforum.org/posts/BbvsoHDoRXuRCs5KZ/recursive-reward-modeling","Jan Leike","jan-leike","167","68","36","2024-02-28T09:25:00.000Z","4800"
"3pinFH3jerMzAvmza","impact-measures-ai-safety","Impact Measures for AI Safety","Designing AI systems to minimize their impact on the world except for achieving their intended objective. Impact measures could provide a form of mild optimization that reduces the risk of unintended consequences.","https://www.alignmentforum.org/posts/3pinFH3jerMzAvmza/impact-measures-ai-safety","Victoria Krakovna","victoria-krakovna","101","42","19","2024-01-30T14:50:00.000Z","3500"
"xWKXcCPYW2gXkxEtZ","value-learning-through-inverse-rl","Value Learning Through Inverse Reinforcement Learning","Using inverse reinforcement learning to infer human values from observed behavior. This post discusses the promise and limitations of IRL for AI alignment, including problems with inferring preferences from suboptimal demonstrations.","https://www.alignmentforum.org/posts/xWKXcCPYW2gXkxEtZ/value-learning-through-inverse-rl","Dylan Hadfield-Menell","dylan-hadfield-menell","119","51","24","2024-01-12T11:15:00.000Z","3900"
"YRgRCYM4gKbWj8Exk","corrigibility-in-ai-systems","Corrigibility in AI Systems","Corrigibility is the property of being safely interruptible and willing to accept changes to one's goals. This post examines why corrigibility is difficult to achieve and various approaches to building corrigible AI systems.","https://www.alignmentforum.org/posts/YRgRCYM4gKbWj8Exk/corrigibility-in-ai-systems","Stuart Armstrong","stuart-armstrong","134","56","29","2023-12-20T16:35:00.000Z","4400"
"8QKHFwE2JXcRKxhfm","transparency-in-ai-development","Transparency in AI Development","The case for transparency in AI development, including open publication of research, disclosure of capabilities, and external auditing. This post weighs the benefits of transparency against information hazards and competitive concerns.","https://www.alignmentforum.org/posts/8QKHFwE2JXcRKxhfm/transparency-in-ai-development","Nick Bostrom","nick-bostrom","156","64","38","2023-11-15T10:20:00.000Z","5200"
"M9cTzLqN4FrdxkHwY","ontology-identification-problem","The Ontology Identification Problem","How can an AI system learn to use the same concepts and categories that humans use? This post explores the challenge of ensuring AI systems carve up the world in human-compatible ways.","https://www.alignmentforum.org/posts/M9cTzLqN4FrdxkHwY/ontology-identification-problem","John Wentworth","john-wentworth","108","44","21","2023-10-08T13:45:00.000Z","3700"
